{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/vi_pydata_virginia_2025/blob/master/notebooks/Pathfinder.ipynb)\n",
    "\n",
    "# Pathfinder Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathfinder (Zhang et al. 2022) offers an elegant solution to many of the common challenges encountered in traditional variational inference. Rather than directly optimizing a variational distribution to match the posterior, Pathfinder constructs a geometric path through the probability space that connects a simple initial distribution to the target posterior. It offers several advantages:\n",
    "\n",
    "1. **Speed**: Requires far fewer gradient evaluations than ADVI or MCMC warmup\n",
    "2. **Scalability**: Performs well on larger problems and high-dimensional models\n",
    "3. **Parallelization**: Can compute ELBO estimates in parallel, unlike ADVI\n",
    "\n",
    "On large problems, it should scale better than most MCMC algorithms, including gradient-based methods like NUTS, and requires 1-2 orders of magnitude fewer log density and gradient evaluations than ADVI and the MCMC warmup phase. Moreover, Pathfinder can perform the Monte Carlo KL divergence estimates used to compute ELBO in parallel, providing a major advantage over ADVI, which must evaluate the ELBO sequentially.\n",
    "\n",
    "Like ADVI, the computational efficiencies may come at the cost of a more biased estimate of the posterior but can be managed through the algorithm's settings. \n",
    "\n",
    "In this tutorial, we'll look at how Pathfinder works conceptually, how to use it in PyMC, and examine its performance across several example problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Pathfinder Works\n",
    "\n",
    "At a high level, Pathfinder works like this:\n",
    "\n",
    "1. **Optimization Path**: Uses L-BFGS optimization to find a good path through the parameter space\n",
    "2. **Local Approximations**: Creates normal (Gaussian) approximations at different points along this path\n",
    "3. **Sample From Approximations**: Monte Carlo samples are drawn from these approximations\n",
    "4. **ELBO Evaluation**: Selects the best approximation using the Evidence Lower Bound (ELBO)\n",
    "5. **Final Sampling**: Draws samples from the best approximation for inference\n",
    "\n",
    "The name \"Pathfinder\" comes from the fact that it finds a good path through parameter space to locate a high-quality approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Optimization Path\n",
    "\n",
    "Pathfinder starts by using a mathematical optimization technique called L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) to move through the parameter space. \n",
    "\n",
    "Think of this like a hiker trying to climb to the top of a mountain by always moving uphill. L-BFGS efficiently tracks the path upward, using information about how steep the slope is (gradients) and how the terrain changes (approximate Hessian matrix).\n",
    "\n",
    "Starting from a random point at $\\theta^{(0)}$, which should be at the tail-region of the posterior distribution, L-BFGS moves through the body of the distribution and towards a local maximum.\n",
    "\n",
    "As the optimization proceeds, Pathfinder records both the positions (parameters) and the local landscape information (gradients) at each step along the path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Local Approximations\n",
    "\n",
    "At each point along this optimization path, Pathfinder creates a normal (Gaussian) approximation to the posterior distribution. This is based on a second-order Taylor expansion:\n",
    "\n",
    "$$ \\log p(\\theta | x) \\approx \\log p(\\theta_0 | x) + g^T(\\theta - \\theta_0) + \\frac{1}{2}(\\theta - \\theta_0)^T H (\\theta - \\theta_0) $$\n",
    "\n",
    "These approximations have:\n",
    "\n",
    "- A **mean** representing the central estimate of the parameters\n",
    "- A **covariance matrix** capturing the uncertainty and relationships between parameters\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu^{(l)} &= \\theta^{(l)} - \\text{H}^{-1}(\\theta^{(l)}) \\cdot g^{(l)} \\\\\n",
    "\\Sigma^{(l)} &= \\text{H}^{-1}(\\theta^{(l)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\text{H}(\\theta^{(l)}) = - \\nabla^2 \\log f(\\theta^{(l)})$ is the approximate Hessian matrix of the log unnormalized density at $\\theta^{(l)}$.\n",
    "\n",
    "Unlike ADVI, where the covariance matrix is either diagonal (mean-field ADVI) or full rank (full-rank ADVI), Pathfinder uses a low-rank plus diagonal factorization of the inverse Hessian factor, where the rank of the estimate for the covariance matrix can be controlled by the user.\n",
    "\n",
    "The covariance matrix is constructed efficiently using information from the optimization path, capturing local curvature of the posterior distribution.\n",
    "\n",
    "### Sampling from Approximations\n",
    "\n",
    "After constructing the inverse Hessian factors along the optimization path, Pathfinder needs to first sample from the resulting normal approximations and then evaluate the log density of these samples. This is necessary to compute the evidence lower bound (ELBO) in the next step, which will allow us to select the best normal approximation.\n",
    "\n",
    "The **BFGS-Sample** algorithm generates samples from a local normal approximation to the target distribution. It takes optimization trajectory points and their gradients, along with inverse Hessian factors, and produces $K$ samples from the corresponding multivariate normal distribution. The algorithm handles two cases: \n",
    "\n",
    "1. when the rank is greater than or equal to the parameter dimension, it uses a direct Cholesky decomposition approach\n",
    "2. otherwise, it employs a more computationally-efficient thin QR factorization method. \n",
    "    \n",
    "In both cases, the algorithm transforms standard normal random variables using the covariance structure encoded in the inverse Hessian factors to produce properly distributed samples, while also computing their log density values for subsequent ELBO calculations.\n",
    "\n",
    "![](images/pathfinder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Best Approximation\n",
    "\n",
    "Once Pathfinder has created these local normal approximations, it needs to decide which one is best. It does this by computing the **Evidence Lower Bound (ELBO)** for each approximation.\n",
    "\n",
    "The ELBO measures how well each approximation matches the true posterior. The higher the ELBO, the better the approximation. This calculation involves:\n",
    "\n",
    "1. Drawing Monte Carlo samples from each normal approximation (from above)\n",
    "2. Evaluating the log density of these samples under both the target distribution and the approximation\n",
    "3. Computing the average difference\n",
    "\n",
    "The approximation with the highest ELBO is selected as the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Sampling\n",
    "\n",
    "Once the best approximation is selected, Pathfinder draws samples from this approximation, re-using the BFGS-Sample function for this. Then, importance sampling is applied to correct for approximation bias.\n",
    "\n",
    "These samples can then be used for posterior inference, just like samples from MCMC methods, but typically at a fraction of the computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling\n",
    "\n",
    "Like all VI methods, Pathfinder is approximating the posterior $p(\\theta|y)$ using a tractable probability density $q(\\theta)$ that is some approximation of $p$. \n",
    "\n",
    "$$E[h(\\theta) | y] = \\frac{\\int h(\\theta) \\frac{p(\\theta|y)}{q(\\theta)} q(\\theta) d\\theta}{\\int \\frac{p(\\theta|y)}{q(\\theta)} q(\\theta) d\\theta}$$\n",
    "\n",
    "Expressed this way, $w(\\theta) = p(\\theta|y) / q(\\theta)$ can be regarded as *weights* for the $M$ values of $\\theta$ sampled from $q$ that we can use to correct the sample so that it approximates $h(\\theta)$. Specifically, the **importance sampling estimate** of $E[h(\\theta) | y]$ is:\n",
    "\n",
    "$$\\hat{h}_{is} = \\frac{\\sum_{i=1}^{M} h(\\theta^{(i)})w(\\theta^{(i)})}{\\sum_{i=1}^{M} w(\\theta^{(i)})}$$\n",
    "\n",
    "where $\\theta^{(i)}$ is the $i^{th}$ sample simulated from $q(\\theta)$. The standard error for the importance sampling estimate is:\n",
    "\n",
    "$$\\text{SE}_{is} = \\frac{\\sqrt{\\sum_{i=1}^{M} [(h(\\theta^{(i)}) - \\hat{h}_{is}) w(\\theta^{(i)})]^2}}{\\sum_{i=1}^{M} w(\\theta^{(i)})}$$\n",
    "\n",
    "The efficiency of importance sampling is related to the selection of the importance sampling distribution $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gamma distribution\n",
    "\n",
    "As a simple illustration of importance sampling, let's consider the problem of estimating a *gamma distribution* using a normal distribution via importance sampling.\n",
    "\n",
    "The goal will be to estimate $E[X]$ where $X \\sim Gamma(3, 2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_true, scale_true = 3.0, 2.0\n",
    "target_mean = shape_true * scale_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll center the normal proposal near the true mean with appropriate variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_proposal, sigma_proposal = 6.0, 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.constrained_layout.use\": True,\n",
    "        \"figure.figsize\": (10, 6),\n",
    "        \"font.size\": 12,\n",
    "    }\n",
    ")\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "np.random.seed(SEED:=42)\n",
    "\n",
    "x = np.linspace(0, 15, 1000)\n",
    "gamma_pdf = stats.gamma.pdf(x, shape_true, scale=scale_true)\n",
    "normal_pdf = stats.norm.pdf(x, mu_proposal, sigma_proposal)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(x, gamma_pdf, 'r-', lw=2, label=f'Target: Gamma({shape_true}, {scale_true})')\n",
    "ax.plot(x, normal_pdf, 'b--', lw=2, label=f'Proposal: Normal({mu_proposal}, {sigma_proposal})')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Target vs. Proposal Distributions')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate candidate samples from the normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10000\n",
    "samples = np.random.normal(mu_proposal, sigma_proposal, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate importance weights:\n",
    "\n",
    "`weights = target_pdf(samples) / proposal_pdf(samples)`\n",
    "\n",
    "which are normalized to sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pdf = stats.gamma.pdf(samples, shape_true, scale=scale_true)\n",
    "proposal_pdf = stats.norm.pdf(samples, mu_proposal, sigma_proposal)\n",
    "weights = target_pdf / proposal_pdf\n",
    "\n",
    "# Remove invalid weights (from sampling in regions where target is defined but proposal isn't)\n",
    "valid_idx = (weights > 0) & np.isfinite(weights)\n",
    "samples = samples[valid_idx]\n",
    "weights = weights[valid_idx]\n",
    "\n",
    "# Normalize weights to sum to 1\n",
    "normalized_weights = weights / np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights are then used to magically convert the normal samples into gamma samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate importance sampling estimate of the mean\n",
    "is_mean = np.sum(samples * normalized_weights)\n",
    "\n",
    "# Calculate standard error of the estimate\n",
    "is_se = np.sqrt(np.sum(((samples - is_mean) * normalized_weights)**2))\n",
    "\n",
    "# Calculate effective sample size\n",
    "ess = 1 / np.sum(normalized_weights**2)\n",
    "\n",
    "print(f\"Importance Sampling Estimate: {is_mean:.4f}\")\n",
    "print(f\"Standard Error: {is_se:.4f}\")\n",
    "print(f\"True Mean: {target_mean}\")\n",
    "print(f\"Effective Sample Size: {ess:.1f} (out of {len(samples)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "bins = np.linspace(0, 15, 50)\n",
    "hist_weights = normalized_weights * len(normalized_weights)  \n",
    "ax.hist(samples, bins=bins, weights=hist_weights, alpha=0.7, \n",
    "         color='skyblue', edgecolor='black', density=True)\n",
    "\n",
    "ax.plot(x, gamma_pdf, 'r-', lw=2, label=f'Target: Gamma({shape_true}, {scale_true})')\n",
    "\n",
    "ax.axvline(target_mean, color='r', linestyle='--', lw=2, label=f'True Mean: {target_mean}')\n",
    "ax.axvline(is_mean, color='g', linestyle='-', lw=2, label=f'IS Mean: {is_mean:.4f}')\n",
    "ax.set_title('Importance Sampling Results')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-path Enhancement\n",
    "\n",
    "The basic Pathfinder algorithm described above is called \"Single-path Pathfinder.\" To improve the quality of approximation, especially for complex posteriors, we can run multiple independent paths and combine their results.\n",
    "\n",
    "This **Multi-path Pathfinder** approach:\n",
    "\n",
    "1. Runs multiple independent Pathfinder instances from different starting points\n",
    "2. Generates $M$ draws from its ELBO-maximizing normal approximation\n",
    "3. Uses importance sampling to combine all draws from all paths\n",
    "4. Selects the final $R$ samples based on their importance weights\n",
    "\n",
    "This is particularly useful for complex posteriors that may have multiple modes or non-normal shapes. By combining samples from different paths, we can better capture the true posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pathfinder in PyMC\n",
    "\n",
    "Let's see how to use Pathfinder for practical Bayesian modeling in PyMC. We'll need to import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pymc_extras as pmx\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "az.style.use(\"arviz-darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: The Eight Schools Problem\n",
    "\n",
    "The Eight Schools problem is a classic example in Bayesian statistics, originally analyzed by Rubin (1981). It involves estimating the effects of coaching programs on SAT scores across eight schools.\n",
    "\n",
    "This is a good starting example because:\n",
    "1. It's small enough to understand easily\n",
    "2. It has a hierarchical structure that creates some posterior complexity\n",
    "3. It's commonly used as a benchmark in Bayesian methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 8  # number of schools\n",
    "y = np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])  # observed effects\n",
    "sigma = np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])  # standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a hierarchical model for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"school\": np.arange(J)}) as model:\n",
    "    baseline = pm.Normal(\"baseline\", mu=0.0, sigma=10.0)\n",
    "    school_sd = pm.HalfCauchy(\"school_sd\", 5.0)\n",
    "\n",
    "    school_offset = pm.Normal(\"school_offset\", mu=0, sigma=1, dims=\"school\")\n",
    "    _ = pm.Normal(\n",
    "        \"obs\",\n",
    "        mu=baseline + school_sd * school_offset,\n",
    "        sigma=sigma,\n",
    "        observed=y,\n",
    "        dims=\"school\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit this model using different methods for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata_nuts = pm.sample(random_seed=SEED)\n",
    "    idata_advi = pm.fit(n=30_000).sample(1000, random_seed=SEED)\n",
    "    idata_pf = pmx.fit(method=\"pathfinder\", num_paths=1, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a helper function to visualize and compare our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods(results, var_names=None):\n",
    "    \"\"\"Plot posterior distributions from different inference methods\"\"\"\n",
    "    az.plot_forest(\n",
    "        [results[key] for key in results.keys()],\n",
    "        model_names=list(results.keys()),\n",
    "        var_names=var_names,\n",
    "        combined=True,\n",
    "        figsize=(10, 10),\n",
    "        kind=\"ridgeplot\",\n",
    "        ridgeplot_alpha=0.5,\n",
    "    )\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = [RV.name for RV in model.free_RVs]\n",
    "res = {\"NUTS\": idata_nuts, \"ADVI\": idata_advi, \"Pathfinder\": idata_pf}\n",
    "\n",
    "compare_methods(res, var_names=var_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Pathfinder Results\n",
    "\n",
    "Pathfinder has several tuning parameters we can adjust to improve its performance:\n",
    "\n",
    "1. **jitter**: Controls how far the starting points are from the initial position\n",
    "2. **num_paths**: Number of independent Pathfinder runs for Multi-path Pathfinder\n",
    "3. **maxcor**: History size for L-BFGS optimization\n",
    "\n",
    "Let's try using Multi-path Pathfinder with more exploratory settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata_jitter_pf = pmx.fit(\n",
    "        method=\"pathfinder\",\n",
    "        jitter=10.0,\n",
    "        random_seed=SEED,\n",
    "    )\n",
    "\n",
    "    idata_jitter_paths_pf = pmx.fit(\n",
    "        method=\"pathfinder\",\n",
    "        num_paths=50,\n",
    "        jitter=10.0,\n",
    "        random_seed=SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"Pathfinder, jitter=10\"] = idata_jitter_pf\n",
    "res[\"Pathfinder, jitter=10, num_paths=50\"] = idata_jitter_paths_pf\n",
    "compare_methods(res, var_names=var_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach for improving model fit is to reparameterize the model. It turns out there is actually room for improvement:\n",
    "\n",
    "1. It is over-parametrized: we have 9 parameters to estimate schools' behavior (one `baseline` + eight `school_offsets`), when there are only 8 schools. We should either constrain one school offset to 0 (aka reference encoding) or constrain the sum of school offsets to 0 (which is enforced by `ZeroSumNormal`. \n",
    "\n",
    "2. The HalfCauchy is a very wide prior on the standard deviation of the population of schools, basically encoding that it's possible that schools are so different from each other that knowing about one doesn't tell us much about the others. This is rarely the case, especially in the social sciences, so using a prior that places less density on very high standard deviations (while still avoiding the problematic value of 0) makes more sense a priori. The following plot shows such a prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preliz as pz\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "\n",
    "pz.HalfCauchy(beta=5).plot_pdf(ax=axes[0], legend=\"title\", pointinterval=True)\n",
    "axes[0].set(xlabel=\"Value\", ylabel=\"Density\")\n",
    "\n",
    "pz.Gamma(alpha=2, beta=2).plot_pdf(ax=axes[1], legend=\"title\", pointinterval=True)\n",
    "axes[1].set(xlabel=\"Value\", ylabel=\"Density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the revised model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"school\": np.arange(J)}) as model:\n",
    "    baseline = pm.Normal(\"baseline\", mu=0.0, sigma=10.0)\n",
    "    school_sd = pm.Gamma(\"school_sd\", 2, 2)\n",
    "    school_offset = pm.ZeroSumNormal(\"school_offset\", dims=\"school\")\n",
    "\n",
    "    _ = pm.Normal(\n",
    "        \"obs\",\n",
    "        mu=baseline + school_sd * school_offset,\n",
    "        sigma=sigma,\n",
    "        observed=y,\n",
    "        dims=\"school\",\n",
    "    )\n",
    "\n",
    "    idata_nuts = pm.sample(nuts_sampler=\"nutpie\", random_seed=SEED)\n",
    "    idata_advi = pm.fit(n=30_000).sample(1000, random_seed=SEED)\n",
    "    idata_pf = pmx.fit(method=\"pathfinder\", num_paths=1, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes about this model:\n",
    "\n",
    "1. We've used `ZeroSumNormal` for the school offsets, which constrains them to sum to zero. This is a better parametrization than the traditional one that can lead to sampling difficulties.\n",
    "\n",
    "2. We're using a `Gamma(2, 2)` prior for `school_sd` instead of a `HalfCauchy(5)`. The Gamma has less density at very high values, reflecting that schools typically aren't extremely different from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = [RV.name for RV in model.free_RVs]\n",
    "res = {\"NUTS\": idata_nuts, \"ADVI\": idata_advi, \"Pathfinder\": idata_pf}\n",
    "\n",
    "compare_methods(res, var_names=var_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Modeling Rugby Match Scores\n",
    "\n",
    "Now let's try Pathfinder on a more complex model. We'll look at a Poisson regression model for rugby match scores, which helps us estimate team attack and defense strengths.\n",
    "\n",
    "The league is made up by a total of T= 6 teams, playing each other once\n",
    "in a season. We indicate the number of points scored by the home and the away team in the g-th game of the season (15 games) as $y_{g1}$ and $y_{g2}$ respectively. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_all = pd.read_csv(\"../data/rugby.csv\", index_col=0)\n",
    "except Exception:\n",
    "    df_all = pd.read_csv(pm.get_data(\"rugby.csv\"), index_col=0)\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_idx, teams = pd.factorize(df_all[\"home_team\"], sort=True)\n",
    "away_idx, _ = pd.factorize(df_all[\"away_team\"], sort=True)\n",
    "coords = {\"match\": df_all.index, \"team\": teams}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector of observed counts $\\mathbb{y} = (y_{g1}, y_{g2})$ is modelled as independent Poisson:\n",
    "$y_{gi}| \\theta_{gj} \\tilde\\;\\;  Poisson(\\theta_{gj})$\n",
    "where the theta parameters represent the scoring intensity in the g-th game for the team playing at home (j=1) and away (j=2), respectively.</p>\n",
    "\n",
    "We model these parameters according to a formulation that has been used widely in the statistical literature, assuming a log-linear random effect model:\n",
    "$$log \\theta_{g1} = home + att_{h(g)} + def_{a(g)} $$\n",
    "$$log \\theta_{g2} = att_{a(g)} + def_{h(g)}$$\n",
    "\n",
    "\n",
    "* The parameter home represents the advantage for the team hosting the game and we assume that this effect is constant for all the teams and throughout the season\n",
    "* The scoring intensity is determined jointly by the attack and defense ability of the two teams involved, represented by the parameters att and def, respectively\n",
    "\n",
    "* Conversely, for each t = 1, ..., T, the team-specific effects are modelled as exchangeable from a common distribution:\n",
    "\n",
    "* $att_{t} \\; \\tilde\\;\\; Normal(\\mu_{att},\\tau_{att})$ and $def_{t} \\; \\tilde\\;\\;Normal(\\mu_{def},\\tau_{def})$\n",
    "\n",
    "* We did some munging above and adjustments of the data to make it **tidier** for our model.\n",
    "* The log function to away scores and home scores is a standard trick in the sports analytics literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as rugby_model:\n",
    "\n",
    "    home_team = pm.Data(\"home_team\", home_idx, dims=\"match\")\n",
    "    away_team = pm.Data(\"away_team\", away_idx, dims=\"match\")\n",
    "\n",
    "    home = pm.Normal(\"home\", mu=0, sigma=1)        # home advantage\n",
    "    sd_att = pm.HalfNormal(\"sd_att\", sigma=2)      # variability in attack\n",
    "    sd_def = pm.HalfNormal(\"sd_def\", sigma=2)      # variability in defense\n",
    "    intercept = pm.Normal(\"intercept\", mu=3, sigma=1)  # baseline scoring rate\n",
    "\n",
    "    atts_star = pm.Normal(\"atts_star\", mu=0, sigma=sd_att, dims=\"team\")\n",
    "    defs_star = pm.Normal(\"defs_star\", mu=0, sigma=sd_def, dims=\"team\")\n",
    "\n",
    "    atts = pm.Deterministic(\"atts\", atts_star - pt.mean(atts_star), dims=\"team\")\n",
    "    defs = pm.Deterministic(\"defs\", defs_star - pt.mean(defs_star), dims=\"team\")\n",
    "    \n",
    "    home_theta = pt.exp(intercept + home + atts[home_idx] + defs[away_idx])\n",
    "    away_theta = pt.exp(intercept + atts[away_idx] + defs[home_idx])\n",
    "\n",
    "    home_points = pm.Poisson(\n",
    "        \"home_points\",\n",
    "        mu=home_theta,\n",
    "        observed=df_all[\"home_score\"],\n",
    "        dims=\"match\",\n",
    "    )\n",
    "    away_points = pm.Poisson(\n",
    "        \"away_points\",\n",
    "        mu=away_theta,\n",
    "        observed=df_all[\"away_score\"],\n",
    "        dims=\"match\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has more parameters than the Eight Schools example, making it a better test for scalability. Let's fit it using different methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rugby_model:\n",
    "    idata_nuts = pm.sample(random_seed=SEED)\n",
    "    idata_advi = pm.fit(n=30_000).sample(1000, random_seed=SEED)\n",
    "    idata_pf = pmx.fit(method=\"pathfinder\", random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = [RV.name for RV in rugby_model.free_RVs]\n",
    "res = {\"NUTS\": idata_nuts, \"ADVI\": idata_advi, \"Pathfinder\": idata_pf}\n",
    "\n",
    "compare_methods(res, var_names=var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rugby_model:\n",
    "    idata_jitter_paths_pf = pmx.fit(\n",
    "        method=\"pathfinder\", jitter=20.0, num_paths=50, random_seed=SEED\n",
    "    )\n",
    "\n",
    "res[\"pf-jitter-paths\"] = idata_jitter_paths_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_methods(res, var_names=var_names)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: The MLB \"Sticky Stuff\" Incident\n",
    "\n",
    "Since the earliest days of the sport, baseball pitchers have applied foreign substances on the ball to help them throw better pitchers. But while it has always been against the rules, is was rarely enforced. In the early days, pine tar was used to allow for a harder grip on the ball, which in turn allows the ball to be spun at a higher rate. Eventually, this evolved to a sticky blend of rosin (powder derived from pine tree sap) and sunscreen. The resulting high spin rates resulted in fewer hits and more strikeouts, and finally led  MLB to a mid-season crackdown in 2021, handing out 10-game suspensions to any pitcher caught using \"sticky stuff\":\n",
    "\n",
    "> Any pitcher who possesses or applies foreign substances will be subject to immediate ejection from the game and suspended automatically in accordance with the rules. If a player other than the pitcher is found to have applied a foreign substance to the ball, both the position player and pitcher will be ejected.\n",
    "\n",
    "![](images/sticky_stuff_scherzer.jpg)\n",
    "\n",
    "With the advent of remote sensing data, it is possible to track the spin rates (and other metrics) of pitched balls. This data is freely available from the [MLB Advanced Media website](https://baseballsavant.mlb.com/). \n",
    "\n",
    "Can we formulate a model to detect any changes in spin rate that are might be attributable to stepped-up enforcement of sticky stuff?\n",
    "\n",
    "> ... word of its arrival trickled out around June 3, as MLB made it known it planned to increase scrutiny amid record-high strikeout rates. (Washington Post)\n",
    "\n",
    "The dataset below includes all curve balls thrown by pitchers during the 2021 season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_date</th>\n",
       "      <th>avg_spin_rate</th>\n",
       "      <th>n_pitches</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitcher_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Wainwright, Adam</th>\n",
       "      <td>2021-04-03</td>\n",
       "      <td>2127.415000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wainwright, Adam</th>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>2179.723000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wainwright, Adam</th>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>2297.968571</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wainwright, Adam</th>\n",
       "      <td>2021-04-20</td>\n",
       "      <td>2159.150000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wainwright, Adam</th>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>2314.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  game_date  avg_spin_rate  n_pitches\n",
       "pitcher_name                                         \n",
       "Wainwright, Adam 2021-04-03    2127.415000         12\n",
       "Wainwright, Adam 2021-04-08    2179.723000         11\n",
       "Wainwright, Adam 2021-04-14    2297.968571          7\n",
       "Wainwright, Adam 2021-04-20    2159.150000         13\n",
       "Wainwright, Adam 2021-04-26    2314.515455         11"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    spin_rates = pd.read_csv('../data/fastball_spin_rates.csv', index_col=0, parse_dates=['game_date'])\n",
    "except FileNotFoundError:\n",
    "    spin_rates = pd.read_csv(pm.get_data('fastball_spin_rates.csv'), index_col=0, \n",
    "parse_dates=['game_date'])\n",
    "spin_rates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kopech_fb_spin = spin_rates.assign(day_of_year=spin_rates.game_date.dt.day_of_year).loc['Kopech, Michael'].copy()\n",
    "kopech_fb_spin.plot.scatter(x='game_date', y='avg_spin_rate', title='Kopech Fastball Spin Rate', ylabel='Spin Rate (rpm)', figsize=(10,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = 5  \n",
    "top_pitchers = spin_rates.groupby(\"pitcher_name\").size().nlargest(n_outputs).reset_index()\n",
    "top_pitchers = top_pitchers.reset_index().rename(columns={\"index\": \"output_idx\", 0: \"games\"})\n",
    "\n",
    "# Plot average spin rates of top pitchers\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "legends = []\n",
    "for pitcher in top_pitchers[\"pitcher_name\"]:\n",
    "    pitcher_data = spin_rates.assign(day_of_year=spin_rates.game_date.dt.day_of_year).loc[pitcher]\n",
    "    ax.scatter(pitcher_data[\"day_of_year\"], pitcher_data[\"avg_spin_rate\"])\n",
    "    legends.append(pitcher)\n",
    "plt.xlabel(\"Day of year\")\n",
    "plt.ylabel(\"Average spin rate (rpm)\")\n",
    "plt.legend(legends, loc=\"upper center\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_subset = spin_rates.assign(day_of_year=spin_rates.game_date.dt.day_of_year).reset_index().merge(top_pitchers, on='pitcher_name', how='right')\n",
    "X = analysis_subset[['day_of_year', 'output_idx']].values\n",
    "y = analysis_subset['avg_spin_rate'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model this set of time series, we will use a Gaussian Process (GP) regression model. GPs are a powerful tool for modeling complex, non-linear relationships in data, and they can provide uncertainty estimates for predictions.\n",
    "\n",
    "### What is a Gaussian Process?\n",
    "\n",
    "A Gaussian Process (GP) is a powerful and flexible statistical tool that extends the concept of multivariate normal distributions to infinite dimensions. At its core, a Gaussian Process defines a probability distribution over functions, rather than just over finite-dimensional vectors. Think of it as a way to specify a \"random function\" where any finite collection of points from this function follows a multivariate normal distribution.\n",
    "\n",
    "Formally, a Gaussian Process is defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. We write:\n",
    "\n",
    "$$f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))$$\n",
    "\n",
    "Where $m(x)$ is the mean function that defines the expected value of $f$ at position $x$:\n",
    "\n",
    "$$m(x) = \\mathbb{E}[f(x)]$$\n",
    "\n",
    "And $k(x, x')$ is the kernel or covariance function that specifies the covariance between any two points:\n",
    "\n",
    "$$k(x, x') = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]$$\n",
    "\n",
    "Why Are Gaussian Processes Useful?\n",
    "Gaussian Processes are particularly valuable for regression and modeling problems because they provide:\n",
    "\n",
    "- Function approximation with uncertainty estimates\n",
    "- Smooth interpolation between observed data points\n",
    "- Principled uncertainty quantification that grows in regions far from observations\n",
    "- Flexibility through different kernel choices that encode prior beliefs about function behavior\n",
    "\n",
    "#### How Do Gaussian Processes Work in Practice?\n",
    "\n",
    "When working with GPs for regression, we typically have observed data points $(x_i, y_i)$ where $y_i = f(x_i) + \\epsilon_i$ and $\\epsilon_i$ is noise. The power of GPs lies in predicting function values $f(x_*)$ at new points $x_*$.\n",
    "\n",
    "Given a set of training points $X$ with observations $y$, and test points $X_*$, the joint distribution of observed values and predictions is:\n",
    "\n",
    "$$\\begin{bmatrix} y | f_* \\end{bmatrix} \\sim \\mathcal{N}\\left(\\begin{bmatrix} m(X) \\ m(X_*) \\end{bmatrix}, \\begin{bmatrix} K(X,X) + \\sigma^2_n I & K(X,X_*) \\ K(X_*,X) & K(X_*,X_*) \\end{bmatrix}\\right)$$\n",
    "\n",
    "After conditioning on the observed data, the predictive distribution becomes:\n",
    "\n",
    "$$f_* | X, y, X_* \\sim \\mathcal{N}(\\bar{f}*, \\text{cov}(f*))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\bar{f}* = m(X) + K(X_, X)[K(X,X) + \\sigma^2_n I]^{-1}(y - m(X))$$ \n",
    "\n",
    "$$\\text{cov}(f_*) = K(X_*,X_*) - K(X_*,X)[K(X,X) + \\sigma^2_n I]^{-1}K(X,X_*)$$\n",
    "\n",
    "#### Choosing a Kernel\n",
    "The kernel function defines the shape and smoothness of functions drawn from the GP. Some common kernels include:\n",
    "\n",
    "- Squared Exponential (RBF): $k(x, x') = \\sigma^2 \\exp\\left(-\\frac{||x-x'||^2}{2\\ell^2}\\right)$ for very smooth functions\n",
    "- Mat√©rn: For functions with different degrees of smoothness\n",
    "- Periodic: $k(x, x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi|x-x'|/p)}{\\ell^2}\\right)$ for repeating patterns\n",
    "\n",
    "The kernel parameters (like length-scale $\\ell$ or output variance $\\sigma^2$) control characteristics of the resulting functions and are typically learned from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as multi_spin_rate_model:\n",
    "    # Priors\n",
    "    ell = pm.Gamma(\"ell\", alpha=2, beta=0.5)\n",
    "    eta = pm.Gamma(\"eta\", alpha=3, beta=1)\n",
    "    K = eta**2 * pm.gp.cov.ExpQuad(input_dim=2, ls=ell, active_dims=[0])\n",
    "\n",
    "    # Get the ICM kernel\n",
    "    W = pm.Normal(\"W\", mu=0, sigma=3, shape=(n_outputs, 2), initval=np.random.randn(n_outputs, 2))\n",
    "    kappa = pm.Gamma(\"kappa\", alpha=1.5, beta=1, shape=n_outputs)\n",
    "    B = pm.Deterministic(\"B\", pt.dot(W, W.T) + pt.diag(kappa))\n",
    "    coreg = pm.gp.cov.Coregion(input_dim=2, B=B, active_dims=[1])\n",
    "    cov_icm = K * coreg  # Use Hadamard Product for separate inputs\n",
    "\n",
    "    # Define a Multi-output GP\n",
    "    gp = pm.gp.Marginal(cov_func=cov_icm)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=3)\n",
    "    f = gp.marginal_likelihood(\"f\", X, y, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(multi_spin_rate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try fitting this model using MCMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multi_spin_rate_model:\n",
    "    multi_trace = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this model will take a while to run, as Gaussian processes are computationally expensive. The time complexity of GP regression is $O(n^3)$, where $n$ is the number of data points. This is due to the need to invert the covariance matrix, which scales cubically with the number of points.\n",
    "\n",
    "Let's try single-path Pathfinder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multi_spin_rate_model:\n",
    "    multi_trace = pmx.fit(method=\"pathfinder\", num_paths=1, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions with a GP, we need to compute the posterior distribution of the function values at new points given the observed data. This involves conditioning the joint Gaussian distribution on the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_pred = np.arange(analysis_subset.day_of_year.min(), analysis_subset.day_of_year.max())\n",
    "pitcher_ind = np.repeat(np.arange(n_outputs), len(days_pred))\n",
    "X_new = np.column_stack((np.tile(days_pred, n_outputs), pitcher_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multi_spin_rate_model:\n",
    "    preds = gp.conditional(\"preds\", X_new)\n",
    "    gp_samples = pm.sample_posterior_predictive(multi_trace, var_names=[\"preds\"], random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc.gp.util import plot_gp_dist\n",
    "\n",
    "f_pred = gp_samples.posterior_predictive[\"preds\"].squeeze()\n",
    "\n",
    "fig, axes = plt.subplots(n_outputs, 1, figsize=(12, 15), sharey=True)\n",
    "\n",
    "M = len(days_pred)\n",
    "\n",
    "for idx, pitcher in enumerate(top_pitchers[\"pitcher_name\"]):\n",
    "    # Prediction\n",
    "    ax = axes[idx]\n",
    "    plot_gp_dist(\n",
    "        ax,\n",
    "        f_pred[:, M * idx : M * (idx + 1)],\n",
    "        X_new[M * idx : M * (idx + 1), 0],\n",
    "        palette=\"Blues\",\n",
    "        fill_alpha=0.1,\n",
    "        samples_alpha=0.1,\n",
    "    )\n",
    "    # Training data points\n",
    "    cond = analysis_subset[\"pitcher_name\"] == pitcher\n",
    "    ax.scatter(analysis_subset.loc[cond, \"day_of_year\"], analysis_subset.loc[cond, \"avg_spin_rate\"], color=\"r\")\n",
    "    ax.set_title(pitcher)\n",
    "\n",
    "fig.supxlabel(\"Day of year\")\n",
    "fig.supylabel(\"Average spin rate (rpm)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Pathfinder\n",
    "\n",
    "Based on our examples, here are some guidelines for when Pathfinder might be the right choice:\n",
    "\n",
    "### Advantages of Pathfinder:\n",
    "\n",
    "1. **Speed**: Pathfinder is typically much faster than MCMC methods, especially for large models.\n",
    "2. **Scalability**: Works well with high-dimensional problems where MCMC might struggle.\n",
    "3. **Initialization**: Can be used to initialize MCMC for faster convergence.\n",
    "4. **Reasonable approximations**: For many models, the approximation quality is good enough for practical use.\n",
    "\n",
    "### When to be cautious:\n",
    "\n",
    "1. **Complex posteriors**: Multi-modal or highly skewed distributions may not be captured well by a small number of normal approximations.\n",
    "2. **High precision requirements**: When you need the most accurate posterior possible and compute time isn't a concern, MCMC might still be preferable.\n",
    "3. **Model diagnostics**: For new or complex models, it's a good idea to compare Pathfinder results with MCMC to ensure good approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Pathfinder\n",
    "\n",
    "If you're using Pathfinder and want to improve its performance, here are some parameters to consider tuning:\n",
    "\n",
    "1. **num_paths**: Increasing this runs multiple independent paths and can help capture more complex posteriors. Try values from 4 (default) to 50 or more for complex problems.\n",
    "\n",
    "2. **jitter**: Controls how far initial points are from each other. Higher values explore more of the parameter space. Default is 2.0, but try 5.0-20.0 for complex models.\n",
    "\n",
    "3. **maxcor**: The history size for L-BFGS optimization. Larger values can help with complex curvature. Default is min(model dimension, 10), but you can try higher values for complex models.\n",
    "\n",
    "4. **num_draws_per_path**: Number of samples drawn from each approximation. Default is 1000, but you might increase for more stable results.\n",
    "\n",
    "5. **importance_sampling**: Method used for combining results across paths. Options are \"psis\" (default) or \"psir\". PSIS generally works better in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Pathfinder offers a compelling approach to variational inference that can significantly speed up Bayesian modeling workflows. It works by finding an optimization path through parameter space, creating normal approximations along the way, and selecting the best one using the ELBO.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. Pathfinder is generally much faster than MCMC methods like NUTS.\n",
    "2. The quality of approximation is often good enough for many practical applications.\n",
    "3. Multi-path Pathfinder with tuned settings can substantially improve results.\n",
    "4. For critical applications, it's good practice to verify Pathfinder results with MCMC.\n",
    "\n",
    "This makes Pathfinder a valuable addition to your Bayesian modeling toolkit, especially when you need quick results for exploratory analysis or with large models where MCMC might be prohibitively slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "1. Zhang, Lu, et al. \"Pathfinder: Parallel quasi-Newton variational inference.\" arXiv preprint arXiv:2108.03782 (2021).\n",
    "2. Rubin, Donald B. \"Estimation in parallel randomized experiments.\" Journal of Educational Statistics 6.4 (1981): 377-401."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
